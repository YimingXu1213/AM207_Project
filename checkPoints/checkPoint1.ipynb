{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meeting Summary \n",
    "Group member: Yuting Kou, Yiming Xu, Yizhou Wang, Ziyi Zhou\n",
    "\n",
    "Project TF: Shu Xu\n",
    "\n",
    "paper: Subspace Inference for Bayesian Deep Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper Content\n",
    "### 1. summary\n",
    "Bayesian neural network often involves high dimensional problem over the parameter space. In this paper, they designed a *Subspace Inference* which conduct dimension reduction over parameter space, then form posterior inference and Bayesian model average. This Subspace Inference shows great performance in terms of accuracy and likelihood and thus implies the low-dim sub-spaces contains a rich representations of full space.\n",
    "\n",
    "### 2. Core: Bayesian Subspace Inference\n",
    "#### 1. Find a subspace\n",
    "1.  idea: find a subspace which contains high density and low-loss weight subspace. $W=\\hat{W}+Pz$\n",
    "    1.  good starting point: pre-trained solution $\\hat{W}=W_{SWA}$\n",
    "2.  methods:\n",
    "    1.  Random Subspace: $P$ is random vector from Normal Distribution\n",
    "    2.  PCA over SGD trajectory: $P$ is PCA components of weight deviation: \n",
    "        <br>SGD snapshot of trajectory: every c iterations\n",
    "    3.  Curve Subspace:\n",
    "        <br>given good solution $w_1,w_2,w_3$, find low-loss curve path by $\\min_{v_1,v_2} Loss_{nn}(w_i \\text{along the span}(v_1,v_2))$\n",
    "3.  performance: Curve Subspace performs the best, but time consuming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.  Conduct Bayesian Inference\n",
    "1.  goal: find posterior $p(z|D)$\n",
    "2.  multiple methods, we use HMC and BBB (Bayes By Backprop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.  Conduct Posterior Predictive\n",
    "1.  goal: evaluate how well the model is by reporting log likelihood and contour plots of posterior in subspace.\n",
    "\n",
    "\n",
    "## Potential interesting questions\n",
    "1. Is there any dimension reduction method which performs in between PCA and Curve subspace?\n",
    "2. Does initializations matters? How sensitive in initialization $w_1,w_2,w_3$ is curve subspace method?\n",
    "3. Change SGD trajectory updates methods from `every c steps` to `low-loss`, what will the performance change? \n",
    "4. Find connections of weight space to functional spaces.\n",
    "\n",
    "\n",
    "## Plan\n",
    "1. Nov.9: basic implementation: \n",
    "    1. Random Subspace\n",
    "    2. PCA\n",
    "    3. HMC and BBB\n",
    "2. Nov.10: decide the directions to explore.\n",
    "3. Nov. 15: Pedagogical Examples over that directions.\n",
    "4. Nov.16: Checkpoints 3: Pedagogical Examples and Rudimentary Implementation\n",
    "5. Dec. 23: implement the research direction. \n",
    "5. Dec. 12-18:paper wrapup\n",
    "    1. write tutorial\n",
    "    2. write our experiments\n",
    "6. Dec. 19 Final deliverable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
