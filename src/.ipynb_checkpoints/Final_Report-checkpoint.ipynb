{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subspace Inference of Bayesian Deep Learning and Sensitivity Testing\n",
    "\n",
    "Yuting Kou, Yizhou Wang, Yiming Xu, Ziyi Zhou\n",
    "\n",
    "## 1. Introduction\n",
    "Our paper’s name is [Subspace Inference for Bayesian Deep Learning](https://arxiv.org/abs/1907.07504).  In this paper, it is trying to solve the following problem:  Bayesian inference has been a  useful  tool  to  perform  inference  with  neural  networks,  but  using  it  could be  challenging  when  the  neural  network  becomes  deeper  or  larger,  since  the parameter space becomes extremely high dimensional and the posterior distribution is highly zig-zag. ![可以放一个这个图](https://i.ibb.co/54NkMcT/zigzag.png).. **需不需要加点文字描述** Common methods can easily fall into local minima or take a long time to converge. How could we still use Bayesian methods to perform inference with those deep neural networks?  This problem is important in the modern world, because all the neural networks are built to be deeper and wider.  These models can have almost perfect predictions, but other than predictions,  we care about the inference on the parameters in them as well.  We want to know what the predictive distribution and uncertaintylook like.  This paper addresses this issue by an approach, in which the original parameter space is reduced into a low-dimensional subspace by some methods, and  then  we  can  perform  Bayesian  inference  over  the  parameters  within  the subspace.\n",
    "\n",
    "This paper lists some related work for which the main points are summarized here:\n",
    "1. The SWAG method can define a distribution over low-dimensional subspace, given the first principal components of the stochastic gradient descent (SGD) iterates.  It's useful in getting a Gaussian posterior over neural network weights.\n",
    "2. Bayesian  inference  could  be  applied  to  probabilistic  PCA  using  projected methods for latent variable models.\n",
    "3. Various ways have been proposed to perform variational inference in a low-dimensional subspace for Bayesian neural network.\n",
    "4. There  is  a  theoretical  guarantee  for  Bayesian  inference  in  the  setting  of constrained posteriors.  The constraints (e.g.  the subspace) can be chosen before or after performing unconstrained inference via SGD.\n",
    "5. Other approaches have tried to perform efficient inference by operating solely in data space, rather than in parameter space.\n",
    "\n",
    "The gap in literature that the paper is trying to fill is that no one previously has proposed the method to approximate Bayesian inference in low-dimensional subspace of the weight space for high-dimensional neural networks.  Therefore, this  method  is  the  authors'  main  contribution  and  they  named  it  `Subspace Inference` (SI)\n",
    "\n",
    "\n",
    "\n",
    "## 2. Technical content\n",
    "\n",
    "### High level\n",
    "Bayesian neural network often involves high dimensional problem over the parameter space. In this paper, they designed a Subspace Inference which conduct dimension reduction over parameter space, then form posterior inference and Bayesian model average. This Subspace Inference shows great performance in terms of accuracy and likelihood and thus implies the low-dimension subspaces contains a rich representations of full space.\n",
    "\n",
    "On the high level, their proposed Bayesian subspace inference algorithm contains three steps:  1) construct subspace, 2) posterior inference within subspace, 3)  form  a  Bayesian  model  average.   During  the  subspace  construction  stage, they  tried  three  subspaces  including  random  subspace,  the  subspace  spanned by the first few PCA components of the SGD trajectory, and curve subspace. They then propose using various methods including the MCMC methods suchas Hamiltonian Monte Carlo (HMC) or elliptical slice sampling (ESS) for the approximate inference procedures\n",
    "\n",
    "### Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "We perform inference in a K-dimensional subspace $\\mathcal{S}$ defined by \n",
    "\\begin{aligned}\n",
    "\\mathcal{S} &= \\{w|w = \\hat{w} + z_1v_1 + \\ldots + z_Kv_K\\} \\\\\n",
    "&= \\{w|w = \\hat{w} + Pz\\}.\n",
    "\\end{aligned}\n",
    "With a fixed $\\hat{w}$ and projection matrix $P$, over which we perform inference, the free parameters of the model are now simply $z\\in\\mathbb{R}^K$. The new model thus has the likelihood function $$p(\\mathcal{D}|z) = p_\\mathcal{M}(\\mathcal{D}|w = \\hat{w} + Pz).$$ We can then perform Bayesian model averaging on new test data points $\\mathcal{D}^*$, using the posterior over the parameters in the subspace:\n",
    "\\begin{aligned}\n",
    "p(\\mathcal{D}^*|\\mathcal{D}) &= \\int p_\\mathcal{M}(\\mathcal{D}^*|\\tilde{w} = \\hat{w} + Pz)p(z|\\mathcal{D})\\text{d}z \\\\\n",
    "&\\approx \\frac{1}{J}\\sum_{j=1}^Jp_\\mathcal{M}(\\mathcal{D}^*|\\tilde{w} = \\hat{w}+P\\tilde{z}_j),\\quad \\tilde{z}_j \\sim p(z|\\mathcal{D}).\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subspace Construction\n",
    "In this section we will show various ways to construct subspace $\\mathcal{S}$.\n",
    "\n",
    "**1. Random Subspaces**\n",
    "\n",
    "The basis $P$ is constructed by ranomly drawing $K$ vectors $v_1, \\ldots, v_K \\sim \\mathcal{N}(0, I_p)$ in th weight space. We then rescale them to have norm 1. The shift vector is generated using the weights of a network pre-trained with stochastic weight averaging (SWA): $\\hat{w} = w_{\\text{SWA}}$. In particular, we run SGD with a high constant learning rate from a pre-trained solution, and form the average $w_{\\text{SWA}} = \\frac{1}{T}\\sum_i w_i$ from the SGD iterates $w_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. PCA of the SGD Trajectory**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Curve Subspaces**\n",
    "\n",
    "In this method, we find paths of near-constant low loss in the weight space between converged SGD solutions starting from different random initializations. Let $w_0, w_1$ be two sets of weights corresponding to the same neural network. Moreover, let $\\phi_\\theta: [0,1] \\rightarrow \\mathbb{R}^d$ be a continuous piecewise smooth parametric curve, with parameters $\\theta$, such that $\\phi_\\theta(0) = w_0, \\phi_\\theta(1) = w_1$. To find a path of high accuracy between $w_0, w_1$, we propose to find parameters $\\theta$ that minimize the expectation over a uniform distribution on the curve, $\\hat{l}(\\theta)$:\n",
    "\\begin{aligned}\n",
    "\\hat{l}(\\theta) &= \\frac{\\int \\mathcal{L}(\\phi_\\theta)\\text{d}\\phi_\\theta}{\\int\\text{d}\\phi_\\theta} = \\frac{\\int_0^1\\mathcal{L}(\\phi_\\theta(t))\\|\\phi_\\theta^{'}(t)\\|\\text{d}t}{\\int_0^1 \\|\\phi_\\theta^{'}(t)\\|\\text{d}t} \\\\\n",
    "&= \\int_0^1 \\mathcal{L}(\\phi_\\theta(t))q_\\theta(t)\\text{d}t = \\mathbb{E}_{t\\sim q_\\theta(t)}\\left[\\mathcal{L}(\\phi_\\theta(t))\\right],\n",
    "\\end{aligned}\n",
    "where the distribution $q_\\theta(t)$ on $t\\in[0, 1]$ is defined as $q_\\theta(t) := \\|\\phi_\\theta^{'}(t)\\|\\cdot\\left(\\int_0^1 \\|\\phi_\\theta^{'}(t)\\|\\text{d}t\\right)^{-1}.$ Note that stochastic gradients of $\\theta$ are generally intractable since $q_\\theta(t)$ depends on $\\theta$. Therefore we will use a more computationally tractable loss:\n",
    "$$\n",
    "l(\\theta) = \\int_0^1 \\mathcal{L}(\\phi_\\theta(t))\\text{d}t = \\mathbb{E}_{t\\sim U(0,1)}\\mathcal{L}(\\phi_\\theta(t)).\n",
    "$$\n",
    "To minimize $l(\\theta)$, at each iteration we sample $\\tilde{t}$ from the unifrom distribution $U(0,1)$ and make a gradient step for $\\theta$ with respect to the loss $\\mathcal{L}(\\phi_\\theta(\\tilde{t}))$. This way we can get unbiased estimates of the gradient of $l(\\theta)$:\n",
    "$$\n",
    "\\nabla_\\theta\\mathcal{L}(\\phi_\\theta(\\tilde{t})) = \\mathbb{E}_{t\\sim U(0,1)} \\nabla_\\theta \\mathcal{L}(\\phi_\\theta(t)) = \\nabla_\\theta \\mathbb{E}_{t\\sim U(0,1)} \\mathcal{L}(\\phi_\\theta(t)) = \\nabla_\\theta l(\\theta).\n",
    "$$\n",
    "Then we can perform gradient descent on $\\theta$ to obtain the optimal curve.\n",
    "\n",
    "\n",
    "In this work we employ the **Polygonal chain**. The trained weights $w_0$ and $w_1$ serve as the endpoints of the chain and the bends of the chain are the parameters $\\theta$ of the curve parametrization. We only consider the simplest case of a chain with one bend $\\theta$. Then\n",
    "$$\n",
    "\\phi_\\theta = \\begin{cases} 2(t\\theta + (0.5-t)w_0), & 0 \\leq t \\leq 0.5 \\\\\n",
    "2((t-0.5)w_1 + (1-t)\\theta), & 0.5 < t \\leq 1. \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments\n",
    "\n",
    "They then compare the subspace inference methods on UCI regression tasks to a variety of methods for approximate Bayesian inference with neural networks. This time the comparison is done quantitatively and they compute Gaussian test likelihood as the metric. They show that subspace inference achieves as good or better test log-likelihoods compared to SGD and SWAG on the original parameter space.\n",
    "\n",
    "\n",
    "Next, they test the proposed method on state-of-the-art convolutional networks on CIFAR datasets. They report the accuracy and negative log-likelihood for each of the subspaces. Going from random, to PCA, to curve subspaces provides progressively better results. It's also shown that subspace inference is competitive with SWAG and consistently outperforms most of the other baselines, including MC-dropout.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "The authors of the paper evaluate subspace inference by combining the three subspaces with different approximate posterior inference methods.  They first compare the performance of each combination on their data and show the comparison visually by drawing the predictive distribution.  From the visualization plots on their data, they find that all the inference methods would work with the random and PCA subspace, but since the curve subspace has a more complex shaped posterior, variational methods do not produce reasonable fits.  The plots also show that the random subspace does not allow growing uncertainty when farther away from the data,  whereas the curve  subspace is the best to adapt uncertainty.\n",
    "\n",
    "\n",
    "They essentially want to prove subspace inference can do just as well as performing inference in the original parameter space, or even better, so they visualize the predictive distributions for simple variational inference applied in the original parameter space. From the visualization, they see the SWAG's predictive distribution is similar to the PCA subspace's one. However, variational inference tends to underfit data, and Gaussian process (GP) with an RBF kernel becomes underconfident when farther away from the neighborhood of data, meaning that uncertainty quickly blows up. It's shown in this procedure that subspace inference might show even better results than original-space inference.\n",
    "\n",
    "\n",
    "It's interesting to see that subspaces can be chosen to contain a diverse variety of representations, corresponding to different high quality predictions. It's also surprising to see that subspace inference can outperform inference in original parameter space sometimes. This work is definitely technically sound as it provides an easier yet promising way for inference with neural networks. The proposed method is practical to use on real data, as people using it only need to train a neural network, find the lower-dimension subspace, perform a much easier inference in the subspace, and convert back to the original parameter space. The experimental section contains enough evidence to support the claims. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Our Extension: Sensitivity Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, we are going to follow the guidance of this paper to conduct our own experiments, to test whether the subspace inference method can be generalized onto a smaller dataset and various hypotheses that are related to the network structure and subspace construction. In our project, we are interested in the stability of this Subspace Inference framework. More specifically, we test its sensitivity from two aspects:\n",
    "1. How does subspace method improve the Bayesian deep neural network in terms of efficiency and uncertainty estimation?\n",
    "2. Does it display similar performance over deep and wide neural network? \n",
    "3. Does it sensitive to the initial value to start with?\n",
    "\n",
    "We also expand PCA subspace with different \"SWA\" methods: instead of take every T steps of the weights, we choose to take snapshot of every local minima of the weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experiments\n",
    "### Code structure\n",
    "\n",
    "Following is our code structure: ![code structure](https://i.ibb.co/ggmrRfY/2.png) We use the unify structure of `model`, `subspace` and `inference` class, and enable the same API usage. \n",
    "\n",
    "### Toy example \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from autograd import numpy as np\n",
    "import time\n",
    "from Inference import Inference\n",
    "from Subspace import Subspace\n",
    "from model import Model\n",
    "import CurveSubspace as curve\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from util import hidecode\n",
    "# hidecode()            % --------- remember to remove comments after finishing all the code. This function can simplify the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 lower bound 65.1166805377315; gradient mag: 164.73210946091575\n",
      "Iteration 100 lower bound 52.86436046756695; gradient mag: 56.95458639088457\n",
      "Iteration 200 lower bound 49.42633385370295; gradient mag: 28.31627548203755\n",
      "Iteration 300 lower bound 47.73326189562314; gradient mag: 18.311751879565836\n",
      "Iteration 400 lower bound 46.647745693540806; gradient mag: 13.47755254054697\n",
      "Iteration 500 lower bound 45.88691542805831; gradient mag: 10.503599898310592\n",
      "Iteration 600 lower bound 45.25067576634; gradient mag: 8.654932255566825\n",
      "Iteration 700 lower bound 30.562858257343013; gradient mag: 24.192575322800902\n",
      "Iteration 800 lower bound 26.97792508592485; gradient mag: 20.354176994190937\n",
      "Iteration 900 lower bound 24.278363637665734; gradient mag: 18.77912591689822\n",
      "Iteration 1000 lower bound 22.029867169404948; gradient mag: 17.427416991221943\n",
      "Iteration 1100 lower bound 20.10610040886032; gradient mag: 16.21045518866593\n",
      "Iteration 1200 lower bound 18.434367471681025; gradient mag: 15.092044558814077\n",
      "Iteration 1300 lower bound 16.967826824623163; gradient mag: 14.051620576729908\n",
      "Iteration 1400 lower bound 15.673763205688546; gradient mag: 13.076506887891881\n",
      "Iteration 1500 lower bound 14.527850196693981; gradient mag: 12.158583034329935\n",
      "Iteration 1600 lower bound 13.511041180892443; gradient mag: 11.292691333772902\n",
      "Iteration 1700 lower bound 12.60773762799294; gradient mag: 10.475901826346682\n",
      "Iteration 1800 lower bound 11.804614404003857; gradient mag: 9.707342852428733\n",
      "Iteration 1900 lower bound 11.08977327305976; gradient mag: 8.988599356616403\n",
      "Iteration 2000 lower bound 10.452003162028152; gradient mag: 8.32495407522392\n",
      "Iteration 2100 lower bound 9.87992684520345; gradient mag: 7.728107510229668\n",
      "Iteration 2200 lower bound 9.360728932888753; gradient mag: 7.221146412450927\n",
      "Iteration 2300 lower bound 8.878118698993092; gradient mag: 6.843182251011429\n",
      "Iteration 2400 lower bound 8.410371873546007; gradient mag: 6.627021939587407\n",
      "Iteration 2500 lower bound 7.937214402172414; gradient mag: 6.461888287390538\n",
      "Iteration 2600 lower bound 7.471807925987624; gradient mag: 6.054967192028709\n",
      "Iteration 2700 lower bound 7.048560738747637; gradient mag: 5.623261826592471\n",
      "Iteration 2800 lower bound 6.657649714610486; gradient mag: 5.301596015489049\n",
      "Iteration 2900 lower bound 6.286937924356821; gradient mag: 5.005817266509643\n",
      "Iteration 3000 lower bound 5.932348941430826; gradient mag: 4.738230196194087\n",
      "Iteration 3100 lower bound 5.591133620620948; gradient mag: 4.496972816298515\n",
      "Iteration 3200 lower bound 5.26127545540811; gradient mag: 4.278187646372165\n",
      "Iteration 3300 lower bound 4.94145795934148; gradient mag: 4.078179065269133\n",
      "Iteration 3400 lower bound 4.6310060561160205; gradient mag: 3.893374295011214\n",
      "Iteration 3500 lower bound 4.329797769584354; gradient mag: 3.720413244585381\n",
      "Iteration 3600 lower bound 4.038157311705764; gradient mag: 3.5562692067094743\n",
      "Iteration 3700 lower bound 3.756740063917853; gradient mag: 3.3983369913380335\n",
      "Iteration 3800 lower bound 3.4864181903710874; gradient mag: 3.244478137088563\n",
      "Iteration 3900 lower bound 3.228173818091557; gradient mag: 3.0930278431838505\n",
      "Iteration 4000 lower bound 2.9830049840819997; gradient mag: 2.9427729212293756\n",
      "Iteration 4100 lower bound 2.7518476624500345; gradient mag: 2.7929113696219305\n",
      "Iteration 4200 lower bound 2.535515098891551; gradient mag: 2.6430034941530747\n",
      "Iteration 4300 lower bound 2.334653699118807; gradient mag: 2.492922187032353\n",
      "Iteration 4400 lower bound 2.1497133142884643; gradient mag: 2.3428066637339153\n",
      "Iteration 4500 lower bound 1.9809292604652153; gradient mag: 2.193020647459891\n",
      "Iteration 4600 lower bound 1.8283137632422373; gradient mag: 2.0441136627500596\n",
      "Iteration 4700 lower bound 1.691655364964971; gradient mag: 1.8967832576590373\n",
      "Iteration 4800 lower bound 1.570525681259662; gradient mag: 1.7518364695730453\n",
      "Iteration 4900 lower bound 1.4642933738374684; gradient mag: 1.610150058676236\n"
     ]
    }
   ],
   "source": [
    "data = np.genfromtxt(r'../example/hw7_data.csv', delimiter=',')[1:]\n",
    "x, y = data[:, 0], data[:, 1]\n",
    "\n",
    "alpha = 1\n",
    "c = 0\n",
    "h = lambda x: np.exp(-alpha * (x - c)**2)\n",
    "\n",
    "###neural network model design choices\n",
    "width = 5\n",
    "hidden_layers = 1\n",
    "input_dim = 1\n",
    "output_dim = 1\n",
    "\n",
    "architecture = {'width': width,\n",
    "               'hidden_layers': hidden_layers,\n",
    "               'input_dim': input_dim,\n",
    "               'output_dim': output_dim,\n",
    "               'activation_fn_type': 'rbf',\n",
    "               'activation_fn_params': 'c=0, alpha=1',\n",
    "               'activation_fn': h}\n",
    "\n",
    "# set random state to make the experiments replicable\n",
    "rand_state = 0\n",
    "random = np.random.RandomState(rand_state)\n",
    "\n",
    "# create a model and train the model\n",
    "my_nn = Model.create(submodel_type=\"Feedforward\", architecture=architecture, Sigma_Y=0.25)\n",
    "\n",
    "# use MSE result as params_init\n",
    "params = {'step_size':1e-3, \n",
    "          'max_iteration':5000, \n",
    "          'random_restarts':1}\n",
    "\n",
    "# fit my neural network to minimize MSE on the given data\n",
    "my_nn.fit(x_train=x.reshape((1, -1)), y_train=y.reshape((1, -1)), params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_test = np.linspace(-10, 10, 1000).reshape(1,-1)\n",
    "y_test = my_nn.forward(x_test, use_subweights = False, weights = my_nn.weights)[0]\n",
    "plt.scatter(x, y, alpha = 0.3)\n",
    "plt.plot(x_test[0], y_test[0], color = 'red')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Subspace.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It costs 6.71 sec to calculate Random Subspace\n"
     ]
    }
   ],
   "source": [
    "begin=time.time()\n",
    "my_subspace_rand = Subspace.create(subspace_type=\"random\", model=my_nn, n_subspace=2)\n",
    "my_subspace_rand.collect_vector(X=x, y=y)\n",
    "end=time.time()\n",
    "print('It costs {:.2f} sec to calculate Random Subspace'.format(end-begin) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA Subspace.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It costs 7.13 sec to calculate PCA Subspace\n"
     ]
    }
   ],
   "source": [
    "begin=time.time()\n",
    "my_subspace_pca = Subspace.create(subspace_type=\"pca\", model=my_nn, n_subspace=2)\n",
    "my_subspace_pca.collect_vector(X=x, y=y)\n",
    "end=time.time()\n",
    "print('It costs {:.2f} sec to calculate PCA Subspace'.format(end-begin) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Curve Subspace.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] loss: 0.234\n",
      "[epoch 21] loss: 0.229\n",
      "[epoch 41] loss: 0.228\n",
      "[epoch 61] loss: 0.226\n",
      "[epoch 81] loss: 0.225\n",
      "[epoch 101] loss: 0.224\n",
      "[epoch 121] loss: 0.217\n",
      "[epoch 141] loss: 0.222\n",
      "[epoch 161] loss: 0.219\n",
      "[epoch 181] loss: 0.219\n"
     ]
    }
   ],
   "source": [
    "def CurveNetGen(w0, w1):\n",
    "    return curve.CurveNet(1, width, hidden_layers, w0, w1)\n",
    "\n",
    "# for training the mid-points\n",
    "params_curve = {'sample_size': 500}\n",
    "net = curve.BaseNet(1, width, hidden_layers)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "dataset = TensorDataset(torch.Tensor(x.reshape(-1, 1)), torch.Tensor(y))\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle = True)\n",
    "\n",
    "# for training the two endpoints\n",
    "#params_base = {'epochs': 1000}\n",
    "params_base = params\n",
    "begin=time.time()\n",
    "my_subspace_curve = Subspace.create(subspace_type=\"curve\", net = net, params_base = params_base, nn = my_nn, \n",
    "                              X = x, y = y, loader = loader, criterion = criterion, \n",
    "                              curve_net_gen = CurveNetGen, params_curve = params_curve)\n",
    "my_subspace_curve.collect_vector(epochs = 200, callback = 20, restart = False)\n",
    "end=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It costs 122.12 sec to calculate Curve Subspace\n"
     ]
    }
   ],
   "source": [
    "print('It costs {:.2f} sec to calculate Curve Subspace'.format(end-begin) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curve subspace is 20 times more than PCA and random subspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**todo:**\n",
    "PCA+RANDOM+(curve) 和HMC+BBB的结果放过来。可以有表格的方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subspace Improves a lot\n",
    "- yiming's Raw dimension performance v.s. subspace.\n",
    "\n",
    "#### Deep and Wide difference\n",
    "- Queena results\n",
    "\n",
    "#### Different initialization\n",
    "start with non_train.weights vs not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA extension\n",
    "先按你这个做法来。不用做新的了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Future Work\n",
    "\n",
    "随便扯几句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
